{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pytwisterx.data import csv_reader\n",
    "from pytwisterx.data import Table\n",
    "from pyarrow import Table as PyArrowTable\n",
    "from pyarrow import Tensor as ArrowTensor\n",
    "import time\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor as TorchTensor\n",
    "from pytwisterx.utils.benchmark import benchmark_with_repitions\n",
    "from pytwisterx.utils.data import MiniBatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train File Path : /home/vibhatha/data/mnist/mnist_train_small.csv\n",
      "Test File Path : /home/vibhatha/data/mnist/mnist_test.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Configurations\n",
    "'''\n",
    "\n",
    "base_path: str = \"/home/vibhatha/data/mnist\"\n",
    "train_file_name: str = \"mnist_train_small.csv\"\n",
    "test_file_name: str = \"mnist_test.csv\"\n",
    "train_file_path: str = os.path.join(base_path, train_file_name)\n",
    "test_file_path: str = os.path.join(base_path, test_file_name)\n",
    "delimiter: str = \",\"\n",
    "\n",
    "'''\n",
    "Timing Configurations:\n",
    "\n",
    "'''\n",
    "reps: int = 10\n",
    "time_data_loading: int = 0\n",
    "time_txtb_to_arrowtb: int = 0\n",
    "time_pyarwtb_to_numpy: int = 0\n",
    "time_numpy_to_arrowtn: int = 0\n",
    "time_numpy_to_torchtn: int = 0\n",
    "time_type: str = \"ms\"\n",
    "\n",
    "'''\n",
    "Check Data Files\n",
    "'''\n",
    "\n",
    "print(\"Train File Path : {}\".format(train_file_path))\n",
    "print(\"Test File Path : {}\".format(test_file_path))\n",
    "\n",
    "assert os.path.exists(train_file_path) == True\n",
    "assert os.path.exists(test_file_path) == True\n",
    "\n",
    "'''\n",
    "Global Vars\n",
    "'''\n",
    "\n",
    "tb_train: Table = None\n",
    "tb_test: Table = None\n",
    "tb_train_arw: PyArrowTable = None\n",
    "tb_test_arw: PyArrowTable = None\n",
    "train_npy: np.ndarray = None\n",
    "test_npy: np.ndarray = None\n",
    "train_arrow_tensor: ArrowTensor = None\n",
    "test_arrow_tensor: ArrowTensor = None\n",
    "train_torch_tensor: TorchTensor = None\n",
    "test_torch_tensor: TorchTensor = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Average Time : 278.8795242 ms\n",
      "Twisterx Table to PyArrow Table Average Time : 44.1993083 ms\n",
      "Pyarrow Table to Numpy Average Time : 64.8584276 ms\n",
      "Numpy to Arrow Tensor Average Time : 0.0043803 ms\n",
      "Numpy to Torch Tensor Average Time : 0.010784199999999999 ms\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load To Twisterx Tables\n",
    "'''\n",
    "\n",
    "\n",
    "@benchmark_with_repitions(repititions=reps, time_type=time_type)\n",
    "def load_data_to_tx_tables():\n",
    "    tb_train: Table = csv_reader.read(train_file_path, delimiter)\n",
    "    tb_test: Table = csv_reader.read(test_file_path, delimiter)\n",
    "    return tb_train, tb_test\n",
    "\n",
    "\n",
    "'''\n",
    "If some pre-processing to do, do it here...\n",
    "Join, shuffle, partition, etc\n",
    "'''\n",
    "\n",
    "\n",
    "@benchmark_with_repitions(repititions=reps, time_type=time_type)\n",
    "def convert_tx_table_to_arrow_table():\n",
    "    tb_train_arw: PyArrowTable = Table.to_arrow(tb_train)\n",
    "    tb_test_arw: PyArrowTable = Table.to_arrow(tb_test)\n",
    "    return tb_train_arw, tb_test_arw\n",
    "\n",
    "\n",
    "@benchmark_with_repitions(repititions=reps, time_type=time_type)\n",
    "def covert_arrow_table_to_numpy():\n",
    "    train_npy: np.ndarray = tb_train_arw.to_pandas().to_numpy()\n",
    "    test_npy: np.ndarray = tb_test_arw.to_pandas().to_numpy()\n",
    "    return train_npy, test_npy\n",
    "\n",
    "\n",
    "@benchmark_with_repitions(repititions=reps, time_type=time_type)\n",
    "def convert_numpy_to_arrow_tensor():\n",
    "    train_arrow_tensor = ArrowTensor.from_numpy(train_npy)\n",
    "    test_arrow_tensor = ArrowTensor.from_numpy(test_npy)\n",
    "    return train_arrow_tensor, test_arrow_tensor\n",
    "\n",
    "\n",
    "@benchmark_with_repitions(repititions=reps, time_type=time_type)\n",
    "def convert_numpy_to_torch_tensor():\n",
    "    train_torch_tensor: TorchTensor = torch.from_numpy(train_npy)\n",
    "    test_torch_tensor: TorchTensor = torch.from_numpy(test_npy)\n",
    "    return train_torch_tensor, test_torch_tensor\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "time_data_loading, (tb_train, tb_test) = load_data_to_tx_tables()\n",
    "time_txtb_to_arrowtb, (tb_train_arw, tb_test_arw) = convert_tx_table_to_arrow_table()\n",
    "time_pyarwtb_to_numpy, (train_npy, test_npy) = covert_arrow_table_to_numpy()\n",
    "time_numpy_to_arrowtn, (train_arrow_tensor, test_arrow_tensor) = convert_numpy_to_arrow_tensor()\n",
    "time_numpy_to_torchtn, (train_torch_tensor, test_torch_tensor) = convert_numpy_to_torch_tensor()\n",
    "\n",
    "print(\"Data Loading Average Time : {} {}\".format(time_data_loading, time_type))\n",
    "print(\"Twisterx Table to PyArrow Table Average Time : {} {}\".format(time_txtb_to_arrowtb, time_type))\n",
    "print(\"Pyarrow Table to Numpy Average Time : {} {}\".format(time_pyarwtb_to_numpy, time_type))\n",
    "print(\"Numpy to Arrow Tensor Average Time : {} {}\".format(time_numpy_to_arrowtn, time_type))\n",
    "print(\"Numpy to Torch Tensor Average Time : {} {}\".format(time_numpy_to_torchtn, time_type))\n",
    "\n",
    "print(\"===========================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "\n",
    "'''\n",
    "Splitting Image Data and Target\n",
    "'''\n",
    "\n",
    "train_data = train_npy[:, 1:785]\n",
    "train_target = train_npy[:, 0]\n",
    "train_target = np.reshape(train_target, (train_target.shape[0], 1))\n",
    "\n",
    "test_data = test_npy[:, 1:785]\n",
    "test_target = test_npy[:, 0]\n",
    "test_target = np.reshape(test_target, (test_target.shape[0], 1))\n",
    "\n",
    "'''\n",
    "Generating Minibatches\n",
    "'''\n",
    "\n",
    "train_data = MiniBatcher.generate_minibatches(data=train_data, minibatch_size=100)\n",
    "train_target = MiniBatcher.generate_minibatches(data=train_target, minibatch_size=100)\n",
    "\n",
    "test_data = MiniBatcher.generate_minibatches(data=test_data, minibatch_size=100)\n",
    "test_target = MiniBatcher.generate_minibatches(data=test_target, minibatch_size=100)\n",
    "\n",
    "'''\n",
    "Data reshaping to match the network config (using original image size)\n",
    "'''\n",
    "\n",
    "train_data = np.reshape(train_data, (train_data.shape[0], train_data.shape[1], img_size, img_size))\n",
    "train_target = np.reshape(train_target, (train_target.shape[0], train_target.shape[1]))\n",
    "\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1], img_size, img_size))\n",
    "test_target = np.reshape(test_target, (test_target.shape[0], test_target.shape[1]))\n",
    "\n",
    "'''\n",
    "Convert Data from Numpy to Torch.Tensor\n",
    "'''\n",
    "\n",
    "train_data = torch.from_numpy(train_data)\n",
    "train_target = torch.from_numpy(train_target)\n",
    "\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_target = torch.from_numpy(test_target)\n",
    "\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sequential Training Algorithm\n",
    "'''\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(file_path=None, stat=\"\"):\n",
    "    \"\"\"\n",
    "    saving the program timing stats\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    fp = open(file_path, mode=\"a+\")\n",
    "    fp.write(stat + \"\\n\")\n",
    "    fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://www.mikulskibartosz.name/how-to-display-a-progress-bar-in-jupyter-notebook/\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress: int=1, message:str=''):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"[{0}] Progress: [{1}] {2:.1f}%\".format(message, \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch(fn,\n",
    "           train_data=None, train_target=None,\n",
    "           test_data=None, test_target=None,\n",
    "           do_log=False):\n",
    "    \"\"\" Initialize the distributed environment.\n",
    "    :param fn: training function\n",
    "    :param backend: Pytorch Backend\n",
    "    :param train_data: training data\n",
    "    :param train_target: training targets\n",
    "    :param test_data: testing data\n",
    "    :param test_target: testing targets\n",
    "    :param do_log: boolean status to log\n",
    "    \"\"\"\n",
    "    # dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    # Setting CUDA FOR TRAINING\n",
    "    # use_cuda = torch.cuda.is_available()\n",
    "    # device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    total_communication_time = 0\n",
    "\n",
    "    local_training_time = time.time()\n",
    "\n",
    "    model = fn(train_data=train_data, train_target=train_target, do_log=False)\n",
    "\n",
    "    local_training_time = time.time() - local_training_time\n",
    "\n",
    "    local_testing_time = time.time()\n",
    "\n",
    "    predict(model=model, device=device, test_data=test_data, test_target=test_target, do_log=do_log)\n",
    "\n",
    "    local_testing_time = time.time() - local_testing_time\n",
    "    print(\"Total Training Time : {}\".format(local_training_time))\n",
    "    print(\"Total Testing Time : {}\".format(local_testing_time))\n",
    "    save_log(\"/tmp/torch_mnist_seq_stats.csv\",\n",
    "             stat=\"{},{},{},{}\".format(1, local_training_time, total_communication_time, local_testing_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, test_data=None, test_target=None, do_log=False):\n",
    "    \"\"\"\n",
    "    testing the trained model\n",
    "    :rtype: None return\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in zip(test_data, test_target):\n",
    "            # total_samples = total_samples + 1\n",
    "            count = count + 1\n",
    "            val1 = len(data)\n",
    "            val2 = len(test_data)\n",
    "            total_samples = (val1 * val2)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = np.reshape(data, (data.shape[0], 1, data.shape[1], data.shape[2])) / 128.0\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target.long(), reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            if (do_log):\n",
    "                print(count, len(data), len(test_data), data.shape, output.shape, correct, total_samples)\n",
    "\n",
    "    test_loss /= (total_samples)\n",
    "    local_accuracy = 100.0 * correct / total_samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, total_samples,\n",
    "        local_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data=None, train_target=None, do_log=False):\n",
    "    \"\"\"\n",
    "    training the MNIST model\n",
    "    :param int world_rank: current processor rank (MPI rank)\n",
    "    :param int world_size: number of processes (MPI world size)\n",
    "    :param tensor train_data: training data as pytorch tensor\n",
    "    :param tensor train_target: training target as pytorch tensor\n",
    "    :param boolean do_log: set logging\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1234)\n",
    "    model = Net()\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=0.01, momentum=0.5)\n",
    "\n",
    "    num_batches = train_data.shape[1]\n",
    "\n",
    "    if (do_log):\n",
    "        print(\"Started Training\")\n",
    "    total_data = len(train_data)\n",
    "    epochs = 5\n",
    "    total_steps = epochs * total_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        count = 0\n",
    "        for data, target in zip(train_data, train_target):\n",
    "            data = np.reshape(data, (data.shape[0], 1, data.shape[1], data.shape[2])) / 128.0\n",
    "            count = count + 1\n",
    "            result = '{0:.4g}'.format((count / float(total_steps)) * 100.0)\n",
    "            # for terminal\n",
    "            #print(\"Progress {}% \\r\".format(result), end='\\r')\n",
    "            # for jupyter notebooks\n",
    "            update_progress(progress=count / total_data, message='Epoch ' + str(epoch))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # this comes with data loading mechanism use target or target.long()\n",
    "            # depending on network specifications.\n",
    "            target = target.long()\n",
    "            loss = F.nll_loss(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch ', epoch, ': ', epoch_loss / num_batches)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Progress: [##########----------] 51.0%\n"
     ]
    }
   ],
   "source": [
    "do_log = False\n",
    "\n",
    "# initialize training\n",
    "launch(fn=train,\n",
    "       train_data=train_data,\n",
    "       train_target=train_target,\n",
    "       test_data=test_data,\n",
    "       test_target=test_target,\n",
    "       do_log=do_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
